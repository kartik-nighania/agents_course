{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/kartik/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: 401 Error, Credentials not correct for https://typewise-206265099952.d.codeartifact.eu-central-1.amazonaws.com/pypi/tw_pypi/simple/pip/\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "agentic-ops\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print(load_dotenv('../../.env'))\n",
    "print(os.environ['LANGSMITH_PROJECT'])\n",
    "os.environ['LANGSMITH_TRACING']=\"true\"\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "QDRANT_URL=\"http://localhost:6333\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "retriever = QdrantVectorStore(\n",
    "            client=QdrantClient(url=QDRANT_URL),\n",
    "            collection_name=\"documentations\",\n",
    "            embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        ).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'filename': 'model_sharing.md', '_id': '15d6194e-4457-40a1-9473-b5a4e86ece82', '_collection_name': 'documentations'}, page_content='library installed. This library allows you to programmatically interact with the Hub.\\\\n\\\\n```bash\\\\n\\\\npip install huggingface_hub\\\\n\\\\nThen use `notebook_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:\\\\n\\\\n>>> from huggingface_hub import notebook_login\\\\n\\\\n>>> notebook_login()\\\\n\\\\n## Convert a model for all frameworks\\\\n\\\\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints. While users are still able to load your model from a different framework if you skip this step, it will be slower because \\\\ud83e\\\\udd17 Transformers will need to convert the checkpoint on-the-fly.\\\\n\\\\nConverting a checkpoint for another framework is easy. Make sure you have PyTorch and TensorFlow installed (see [here](installation) for installation instructions), and then find the specific model for your task in the other framework. \\\\n\\\\nSpecify `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch:\\\\n\\\\n>>> pt_model = DistilBertForSequenceClassification.from_pretrained(\\\\\"path/to/awesome-name-you-picked\\\\\", from_tf=True)\\\\n\\\\n>>> pt_model.save_pretrained(\\\\\"path/to/awesome-name-you-picked\\\\\")\\\\n\\\\nSpecify `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow:\\\\n\\\\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\\\\\"path/to/awesome-name-you-picked\\\\\", from_pt=True)\\\\n\\\\nThen you can save your new TensorFlow model with its new checkpoint:\\\\n\\\\n>>> tf_model.save_pretrained(\\\\\"path/to/awesome-name-you-picked\\\\\")\\\\n\\\\nIf a model is available in Flax, you can also convert a checkpoint from PyTorch to Flax:\\\\n\\\\n>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\\\\n\\\\n     \\\\\"path/to/awesome-name-you-picked\\\\\", from_pt=True\\\\n\\\\n )\\\\n\\\\n## Push a model during training\\\\n\\\\nSharing a model to the Hub is as simple as adding an extra parameter or callback. Remember from the [fine-tuning tutorial](training), the [`TrainingArguments`] class is where you specify hyperparameters and additional training options. One of these training options includes the ability to push a model directly to the Hub. Set `push_to_hub=True` in your [`TrainingArguments`]:\\\\n\\\\n>>> training_args = TrainingArguments(output_dir=\\\\\"my-awesome-model\\\\\", push_to_hub=True)\\\\n\\\\nPass your training arguments as usual to [`Trainer`]:\\\\n\\\\n>>> trainer =')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is huggingface ? \")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return OpenAI().chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is used for sharing and accessing machine learning models, particularly in natural language processing and other AI tasks. It provides a platform (Model Hub) where users can upload, download, and collaborate on models, facilitating easier model deployment and experimentation. Additionally, it offers libraries like Transformers for building and fine-tuning models across various frameworks.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is huggingface used for?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"sample_runtime\": \"metadata added\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
