{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/kartik/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: 401 Error, Credentials not correct for https://typewise-206265099952.d.codeartifact.eu-central-1.amazonaws.com/pypi/tw_pypi/simple/pip/\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "agentic-ops\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print(load_dotenv('../../.env'))\n",
    "print(os.environ['LANGSMITH_PROJECT'])\n",
    "os.environ['LANGSMITH_TRACING']=\"true\"\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "QDRANT_URL=\"http://localhost:6333\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "retriever = QdrantVectorStore(\n",
    "            client=QdrantClient(url=QDRANT_URL),\n",
    "            collection_name=\"documentations\",\n",
    "            embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        ).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['fa6fba53-3bf8-445c-b9b3-f01cd4493d36',\n",
       "  '10130a73-a232-4b34-b370-d37e6ddca4b6',\n",
       "  'df1aeb36-777c-4028-a809-0f9cb6ed50ee',\n",
       "  '3bf8162d-0619-41ee-b439-9347b8bf2885',\n",
       "  'beeb1981-f60c-44d3-ad66-e880852f0b59',\n",
       "  '8720b535-614d-4b4f-9b76-1334aa1e5ec8',\n",
       "  'c8c05254-8878-4af1-94fe-677a4c023670',\n",
       "  'fd3e7f80-2f2d-4970-986a-6de8ca83d071',\n",
       "  '9da67a80-98c3-47ac-adb0-f7066b5fa192',\n",
       "  'bf8f4d54-4e79-410c-bbdb-a9dbf53161bc',\n",
       "  '8fc42409-e3bb-4a30-bc7f-2d2ed748e465'],\n",
       " 'count': 11}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_inputs = [\n",
    "    (\"How do I upload a model to the Hugging Face Hub?\", \"To upload a model to the Hugging Face Hub, first install the huggingface_hub library with `pip install huggingface_hub`. Then, use the `login` function with your auth token: `huggingface_hub.login()`. Next, create a model repository with `create_repo()` and use `push_to_hub()` from the appropriate model class. For Transformers models, most model classes have the `push_to_hub()` method built-in. For custom models, you can use `huggingface_hub.upload_file()` or `huggingface_hub.upload_folder()` to upload your model files.\"),\n",
    "\n",
    "    (\"What is the Hugging Face Hub?\", \"The Hugging Face Hub is a platform for sharing and discovering machine learning models, datasets, and demo applications. It serves as a central repository where users can freely access thousands of pre-trained models across various domains like NLP, computer vision, and audio processing. The Hub enables version control for machine learning assets, provides model documentation through model cards, offers a collaborative environment for the AI community, and integrates seamlessly with popular ML frameworks like PyTorch and TensorFlow.\"),\n",
    "\n",
    "    (\"How do I fine-tune a pre-trained model from Hugging Face?\", \"To fine-tune a pre-trained model from Hugging Face, first install the transformers library with `pip install transformers`. Next, load the pre-trained model and tokenizer using `from_pretrained()`. Prepare your dataset and process it with the tokenizer. Then, initialize a `Trainer` object with your model, training arguments, and datasets. Finally, call `trainer.train()` to start fine-tuning. You can also use the `SFTTrainer` from TRL library for more specialized supervised fine-tuning, especially for language models. After training, save your model with `save_pretrained()` or push it to the Hub with `push_to_hub()`.\"),\n",
    "\n",
    "    (\"How do I use Accelerate for distributed training with Hugging Face?\", \"To use Accelerate for distributed training with Hugging Face, start by installing it with `pip install accelerate`. Initialize your model, optimizer, and dataloaders normally. Wrap them with Accelerate by creating an accelerator object (`accelerator = Accelerate()`) and then using `model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)`. Write your training loop as usual, but use `accelerator.backward(loss)` instead of `loss.backward()`. Accelerate automatically handles device placement, gradient synchronization, and mixed precision training across multiple GPUs or TPUs. You can launch distributed training with the `accelerate launch` command.\"),\n",
    "\n",
    "    (\"What is the difference between Tokenizers and Transformers libraries?\", \"The Tokenizers library is specialized for fast text tokenization, focusing on performance and customization of the tokenization process. It provides implementations of popular tokenization algorithms with a Rust backend for speed. The Transformers library, on the other hand, is a comprehensive framework for using pre-trained transformer models, including loading, fine-tuning, and inference with models like BERT, GPT, T5, etc. While Transformers includes tokenization capabilities, the dedicated Tokenizers library offers more advanced features like parallel processing, memory mapping, and customizable components (normalizers, pre-tokenizers, etc.) for building specialized tokenization pipelines.\"),\n",
    "\n",
    "    (\"How do I create a custom dataset with the Datasets library?\", \"To create a custom dataset with the Hugging Face Datasets library, you have several options. For local files, use `load_dataset('csv', data_files='path/to/file.csv')` or similar functions based on your file format. For custom data structures, create a dictionary of lists with `Dataset.from_dict({'text': [...], 'label': [...]})`. For more complex scenarios, define a generator function that yields examples and use `Dataset.from_generator(generator_function)`. You can also create a dataset script by defining `_info()`, `_split_generators()`, and `_generate_examples()` functions, and then placing it in the Datasets library's scripts directory or passing it directly to `load_dataset()`. After creation, you can save your dataset locally with `dataset.save_to_disk()` or share it on the Hub with `dataset.push_to_hub()`.\"),\n",
    "\n",
    "    (\"How do I enable mixed precision training with Hugging Face?\", \"To enable mixed precision training with Hugging Face, you have multiple options. With the Trainer API, simply set `fp16=True` in your TrainingArguments: `training_args = TrainingArguments(fp16=True, ...)`. If using Accelerate, initialize it with `accelerator = Accelerate(mixed_precision='fp16')` or set the configuration via `accelerate config`. For manual training loops with PyTorch, you can use `torch.cuda.amp.autocast()` context manager around your forward pass and scale gradients with `torch.cuda.amp.GradScaler()`. Alternatively, for even easier setup with any training loop, use the `deepspeed` integration by specifying a DeepSpeed config file in your TrainingArguments or Accelerate setup.\"),\n",
    "\n",
    "    (\"What is a model card and how do I create one?\", \"A model card is a documentation file providing essential information about a machine learning model, including its intended use, limitations, training data, evaluation results, and ethical considerations. To create a model card on Hugging Face Hub, create a README.md file in your model repository with structured information following the model card template. Include sections like Model Description, Intended Uses, Training Data, Evaluation Results, Limitations, and Bias and Fairness considerations. You can use the `modelcards` library with `pip install modelcards` to programmatically create model cards using Python. The card should help users understand when and how to use your model appropriately, along with its performance characteristics and potential risks.\"),\n",
    "\n",
    "    (\"How do I use PEFT for efficient fine-tuning?\", \"To use PEFT (Parameter-Efficient Fine-Tuning) with Hugging Face, first install the library with `pip install peft`. Load your pre-trained model with `model = AutoModelForCausalLM.from_pretrained(\\\"model_name\\\")`. Configure your PEFT method by creating a configuration, such as `config = LoraConfig(...)` for LoRA fine-tuning. Wrap your model with the PEFT wrapper using `model = get_peft_model(model, config)`. Then proceed with training as usual, either with the Trainer API or your custom training loop. PEFT supports multiple efficient fine-tuning methods including LoRA, QLoRA, Prefix Tuning, P-Tuning, and Prompt Tuning, allowing you to fine-tune large models with significantly reduced memory requirements by training only a small subset of parameters.\"),\n",
    "\n",
    "    (\"How do I quantize a model with bitsandbytes in Hugging Face?\", \"To quantize a model with bitsandbytes in Hugging Face, first install the bitsandbytes library with `pip install bitsandbytes`. Then, when loading your model with the `from_pretrained()` method, specify the quantization parameters by adding `load_in_8bit=True` or `load_in_4bit=True` arguments, along with other options like `llm_int8_threshold` or `bnb_4bit_compute_dtype`. For example: `model = AutoModelForCausalLM.from_pretrained(\\\"model_name\\\", load_in_4bit=True, bnb_4bit_quant_type=\\\"nf4\\\")`. This allows loading large models with significantly reduced memory usage. For more advanced quantization configurations, you can use `BitsAndBytesConfig` to specify detailed parameters like: `model = AutoModelForCausalLM.from_pretrained(\\\"model_name\\\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))`.\"),\n",
    "\n",
    "    (\"How do I create a Gradio demo for my Hugging Face model?\", \"To create a Gradio demo for your Hugging Face model, first install Gradio with `pip install gradio`. Load your model and tokenizer with the Transformers library. Define a prediction function that takes user inputs, processes them with your model, and returns the results. Then use Gradio's interface creation functions to build your UI, connecting your prediction function to appropriate input and output components. For example, `demo = gr.Interface(fn=predict, inputs=\\\"text\\\", outputs=\\\"text\\\")`. Finally, launch your demo with `demo.launch()`. You can customize the interface with various input types (text, image, audio), output visualizations, examples, and styling options. To deploy on Hugging Face Spaces, create a Space on the Hub, add your demo code in an app.py file, and include requirements.txt with necessary dependencies.\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "# TODO\n",
    "dataset_id = \"c238d5fb-3c75-4f2d-a84a-e8c667dddd25\"\n",
    "\n",
    "# Prepare inputs and outputs for bulk creation\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]\n",
    "outputs = [{\"output\": output_answer} for _, output_answer in example_inputs]\n",
    "\n",
    "client.create_examples(\n",
    "  inputs=inputs,\n",
    "  outputs=outputs,\n",
    "  dataset_id=dataset_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hugging Face's Accelerate is a library designed to simplify the process of training models in a distributed setup, whether on multiple GPUs in one machine or across several machines. To use it, you need to install the library, create an `Accelerator` object, prepare your training objects (like DataLoaders, model, and optimizer), and replace the typical `loss.backward()` with `accelerator.backward(loss)` in your training loop. For more detailed instructions, you can refer to the [documentation](https://huggingface.co/docs/accelerate).\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Hugging accelerator and how do I use it?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
